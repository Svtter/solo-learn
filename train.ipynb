{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d662a5b7-35b7-413b-b435-dcde0de6c175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train mae test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "516ee496-cd99-4834-9797-3e93dc010f23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/conda/lib/python3.11/site-packages/timm/models/helpers.py:7: FutureWarning: Importing from timm.models.helpers is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
      "/opt/conda/lib/python3.11/site-packages/timm/optim/optim_factory.py:7: FutureWarning: Importing from timm.optim.optim_factory is deprecated, please import via timm.optim\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.optim\", FutureWarning)\n",
      "/opt/conda/lib/python3.11/site-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
      "/workspace/solo/backbones/convnext/convnext.py:26: UserWarning: Overwriting convnext_tiny in registry with solo.backbones.convnext.convnext.convnext_tiny. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  @register_model\n",
      "/workspace/solo/backbones/convnext/convnext.py:33: UserWarning: Overwriting convnext_small in registry with solo.backbones.convnext.convnext.convnext_small. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  @register_model\n",
      "/workspace/solo/backbones/convnext/convnext.py:40: UserWarning: Overwriting convnext_base in registry with solo.backbones.convnext.convnext.convnext_base. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  @register_model\n",
      "/workspace/solo/backbones/convnext/convnext.py:47: UserWarning: Overwriting convnext_large in registry with solo.backbones.convnext.convnext.convnext_large. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  @register_model\n",
      "/opt/conda/lib/python3.11/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "/workspace/solo/backbones/poolformer/poolformer.py:421: UserWarning: Overwriting poolformer_s12 in registry with solo.backbones.poolformer.poolformer.poolformer_s12. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  @register_model\n",
      "/workspace/solo/backbones/poolformer/poolformer.py:446: UserWarning: Overwriting poolformer_s24 in registry with solo.backbones.poolformer.poolformer.poolformer_s24. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  @register_model\n",
      "/workspace/solo/backbones/poolformer/poolformer.py:467: UserWarning: Overwriting poolformer_s36 in registry with solo.backbones.poolformer.poolformer.poolformer_s36. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  @register_model\n",
      "/workspace/solo/backbones/poolformer/poolformer.py:489: UserWarning: Overwriting poolformer_m36 in registry with solo.backbones.poolformer.poolformer.poolformer_m36. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  @register_model\n",
      "/workspace/solo/backbones/poolformer/poolformer.py:511: UserWarning: Overwriting poolformer_m48 in registry with solo.backbones.poolformer.poolformer.poolformer_m48. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  @register_model\n",
      "/workspace/solo/utils/whitening.py:43: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd(cast_inputs=torch.float32)\n",
      "/workspace/solo/utils/whitening.py:220: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd(cast_inputs=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "# Copyright 2023 solo-learn development team.\n",
    "\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy of\n",
    "# this software and associated documentation files (the \"Software\"), to deal in\n",
    "# the Software without restriction, including without limitation the rights to use,\n",
    "# copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the\n",
    "# Software, and to permit persons to whom the Software is furnished to do so,\n",
    "# subject to the following conditions:\n",
    "\n",
    "# The above copyright notice and this permission notice shall be included in all copies\n",
    "# or substantial portions of the Software.\n",
    "\n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED,\n",
    "# INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR\n",
    "# PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE\n",
    "# FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR\n",
    "# OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n",
    "# DEALINGS IN THE SOFTWARE.\n",
    "\n",
    "import inspect\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import hydra\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from lightning.pytorch import Trainer\n",
    "from lightning.pytorch.callbacks import LearningRateMonitor\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from lightning.pytorch.strategies.ddp import DDPStrategy\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from timm.data.mixup import Mixup\n",
    "from timm.loss import LabelSmoothingCrossEntropy, SoftTargetCrossEntropy\n",
    "\n",
    "from solo.args.linear import parse_cfg\n",
    "from solo.data.classification_dataloader import prepare_data\n",
    "from solo.methods.base import BaseMethod\n",
    "from solo.methods.linear import LinearModel\n",
    "from solo.utils.auto_resumer import AutoResumer\n",
    "from solo.utils.checkpointer import Checkpointer\n",
    "from solo.utils.misc import make_contiguous\n",
    "\n",
    "try:\n",
    "    from solo.data.dali_dataloader import ClassificationDALIDataModule\n",
    "except ImportError:\n",
    "    _dali_avaliable = False\n",
    "else:\n",
    "    _dali_avaliable = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa7ebb52-2cb3-4395-a2c2-f11aaca5a8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@hydra.main(version_base=\"1.2\")\n",
    "def main(cfg: DictConfig):\n",
    "    # hydra doesn't allow us to add new keys for \"safety\"\n",
    "    # set_struct(..., False) disables this behavior and allows us to add more parameters\n",
    "    # without making the user specify every single thing about the model\n",
    "    OmegaConf.set_struct(cfg, False)\n",
    "    cfg = parse_cfg(cfg)\n",
    "\n",
    "    backbone_model = BaseMethod._BACKBONES[cfg.backbone.name]\n",
    "\n",
    "    # initialize backbone\n",
    "    backbone = backbone_model(method=cfg.pretrain_method, **cfg.backbone.kwargs)\n",
    "    if cfg.backbone.name.startswith(\"resnet\"):\n",
    "        # remove fc layer\n",
    "        backbone.fc = nn.Identity()\n",
    "        cifar = cfg.data.dataset in [\"cifar10\", \"cifar100\"]\n",
    "        if cifar:\n",
    "            backbone.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=2, bias=False)\n",
    "            backbone.maxpool = nn.Identity()\n",
    "\n",
    "    ckpt_path = cfg.pretrained_feature_extractor\n",
    "    assert ckpt_path.endswith(\".ckpt\") or ckpt_path.endswith(\".pth\") or ckpt_path.endswith(\".pt\")\n",
    "\n",
    "    state = torch.load(ckpt_path, map_location=\"cpu\")[\"state_dict\"]\n",
    "    for k in list(state.keys()):\n",
    "        if \"encoder\" in k:\n",
    "            state[k.replace(\"encoder\", \"backbone\")] = state[k]\n",
    "            logging.warn(\n",
    "                \"You are using an older checkpoint. Use a new one as some issues might arrise.\"\n",
    "            )\n",
    "        if \"backbone\" in k:\n",
    "            state[k.replace(\"backbone.\", \"\")] = state[k]\n",
    "        del state[k]\n",
    "    backbone.load_state_dict(state, strict=False)\n",
    "    logging.info(f\"Loaded {ckpt_path}\")\n",
    "\n",
    "    # check if mixup or cutmix is enabled\n",
    "    mixup_func = None\n",
    "    mixup_active = cfg.mixup > 0 or cfg.cutmix > 0\n",
    "    if mixup_active:\n",
    "        logging.info(\"Mixup activated\")\n",
    "        mixup_func = Mixup(\n",
    "            mixup_alpha=cfg.mixup,\n",
    "            cutmix_alpha=cfg.cutmix,\n",
    "            cutmix_minmax=None,\n",
    "            prob=1.0,\n",
    "            switch_prob=0.5,\n",
    "            mode=\"batch\",\n",
    "            label_smoothing=cfg.label_smoothing,\n",
    "            num_classes=cfg.data.num_classes,\n",
    "        )\n",
    "        # smoothing is handled with mixup label transform\n",
    "        loss_func = SoftTargetCrossEntropy()\n",
    "    elif cfg.label_smoothing > 0:\n",
    "        loss_func = LabelSmoothingCrossEntropy(smoothing=cfg.label_smoothing)\n",
    "    else:\n",
    "        loss_func = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    model = LinearModel(backbone, loss_func=loss_func, mixup_func=mixup_func, cfg=cfg)\n",
    "    make_contiguous(model)\n",
    "    # can provide up to ~20% speed up\n",
    "    if not cfg.performance.disable_channel_last:\n",
    "        model = model.to(memory_format=torch.channels_last)\n",
    "\n",
    "    if cfg.data.format == \"dali\":\n",
    "        val_data_format = \"image_folder\"\n",
    "    else:\n",
    "        val_data_format = cfg.data.format\n",
    "\n",
    "    train_loader, val_loader = prepare_data(\n",
    "        cfg.data.dataset,\n",
    "        train_data_path=cfg.data.train_path,\n",
    "        val_data_path=cfg.data.val_path,\n",
    "        data_format=val_data_format,\n",
    "        batch_size=cfg.optimizer.batch_size,\n",
    "        num_workers=cfg.data.num_workers,\n",
    "        auto_augment=cfg.auto_augment,\n",
    "    )\n",
    "\n",
    "    if cfg.data.format == \"dali\":\n",
    "        assert (\n",
    "            _dali_avaliable\n",
    "        ), \"Dali is not currently avaiable, please install it first with pip3 install .[dali].\"\n",
    "\n",
    "        assert not cfg.auto_augment, \"Auto augmentation is not supported with Dali.\"\n",
    "\n",
    "        dali_datamodule = ClassificationDALIDataModule(\n",
    "            dataset=cfg.data.dataset,\n",
    "            train_data_path=cfg.data.train_path,\n",
    "            val_data_path=cfg.data.val_path,\n",
    "            num_workers=cfg.data.num_workers,\n",
    "            batch_size=cfg.optimizer.batch_size,\n",
    "            data_fraction=cfg.data.fraction,\n",
    "            dali_device=cfg.dali.device,\n",
    "        )\n",
    "\n",
    "        # use normal torchvision dataloader for validation to save memory\n",
    "        dali_datamodule.val_dataloader = lambda: val_loader\n",
    "\n",
    "    # 1.7 will deprecate resume_from_checkpoint, but for the moment\n",
    "    # the argument is the same, but we need to pass it as ckpt_path to trainer.fit\n",
    "    ckpt_path, wandb_run_id = None, None\n",
    "    if cfg.auto_resume.enabled and cfg.resume_from_checkpoint is None:\n",
    "        auto_resumer = AutoResumer(\n",
    "            checkpoint_dir=os.path.join(cfg.checkpoint.dir, \"linear\"),\n",
    "            max_hours=cfg.auto_resume.max_hours,\n",
    "        )\n",
    "        resume_from_checkpoint, wandb_run_id = auto_resumer.find_checkpoint(cfg)\n",
    "        if resume_from_checkpoint is not None:\n",
    "            print(\n",
    "                \"Resuming from previous checkpoint that matches specifications:\",\n",
    "                f\"'{resume_from_checkpoint}'\",\n",
    "            )\n",
    "            ckpt_path = resume_from_checkpoint\n",
    "    elif cfg.resume_from_checkpoint is not None:\n",
    "        ckpt_path = cfg.resume_from_checkpoint\n",
    "        del cfg.resume_from_checkpoint\n",
    "\n",
    "    callbacks = []\n",
    "\n",
    "    if cfg.checkpoint.enabled:\n",
    "        ckpt = Checkpointer(\n",
    "            cfg,\n",
    "            logdir=os.path.join(cfg.checkpoint.dir, \"linear\"),\n",
    "            frequency=cfg.checkpoint.frequency,\n",
    "            keep_prev=cfg.checkpoint.keep_prev,\n",
    "        )\n",
    "        callbacks.append(ckpt)\n",
    "\n",
    "    # wandb logging\n",
    "    if cfg.wandb.enabled:\n",
    "        wandb_logger = WandbLogger(\n",
    "            name=cfg.name,\n",
    "            project=cfg.wandb.project,\n",
    "            entity=cfg.wandb.entity,\n",
    "            offline=cfg.wandb.offline,\n",
    "            resume=\"allow\" if wandb_run_id else None,\n",
    "            id=wandb_run_id,\n",
    "        )\n",
    "        wandb_logger.watch(model, log=\"gradients\", log_freq=100)\n",
    "        wandb_logger.log_hyperparams(OmegaConf.to_container(cfg))\n",
    "\n",
    "        # lr logging\n",
    "        lr_monitor = LearningRateMonitor(logging_interval=\"step\")\n",
    "        callbacks.append(lr_monitor)\n",
    "\n",
    "    trainer_kwargs = OmegaConf.to_container(cfg)\n",
    "    # we only want to pass in valid Trainer args, the rest may be user specific\n",
    "    valid_kwargs = inspect.signature(Trainer.__init__).parameters\n",
    "    trainer_kwargs = {name: trainer_kwargs[name] for name in valid_kwargs if name in trainer_kwargs}\n",
    "    trainer_kwargs.update(\n",
    "        {\n",
    "            \"logger\": wandb_logger if cfg.wandb.enabled else None,\n",
    "            \"callbacks\": callbacks,\n",
    "            \"enable_checkpointing\": False,\n",
    "            \"strategy\": DDPStrategy(find_unused_parameters=False)\n",
    "            if cfg.strategy == \"ddp\"\n",
    "            else cfg.strategy,\n",
    "        }\n",
    "    )\n",
    "    trainer = Trainer(**trainer_kwargs)\n",
    "\n",
    "    if cfg.data.format == \"dali\":\n",
    "        trainer.fit(model, ckpt_path=ckpt_path, datamodule=dali_datamodule)\n",
    "    else:\n",
    "        trainer.fit(model, train_loader, val_loader, ckpt_path=ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f09acff-e549-4f82-b64c-4eb63664cea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [--help] [--hydra-help] [--version]\n",
      "                             [--cfg {job,hydra,all}] [--resolve]\n",
      "                             [--package PACKAGE] [--run] [--multirun]\n",
      "                             [--shell-completion] [--config-path CONFIG_PATH]\n",
      "                             [--config-name CONFIG_NAME]\n",
      "                             [--config-dir CONFIG_DIR]\n",
      "                             [--experimental-rerun EXPERIMENTAL_RERUN]\n",
      "                             [--info [{all,config,defaults,defaults-tree,plugins,searchpath}]]\n",
      "                             [overrides ...]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b56ab4e-801e-47b6-b8a3-8ba007b8e7c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
